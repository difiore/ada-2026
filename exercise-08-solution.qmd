# Exercise 08 Solution {.unnumbered}

# â€¢ Solution {.unnumbered}

### Step 1 {.unnumbered}

Using the {tidyverse} `read_csv()` function, load the "Street_et_al_2017.csv" dataset as a "tibble" named **d**.

```{r}
#| warning: false
#| message: false
library(tidyverse)
library(broom)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/Street_et_al_2017.csv"
d <- read_csv(f, col_names = TRUE)
```

Do a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.

```{r}
#| warning: false
#| message: false
library(skimr)
library(kableExtra)
skim(d) |>
  kable() |>
  kable_styling(font_size = 10, full_width = FALSE)
detach(package:kableExtra)
detach(package:skimr)
```

### Step 2 {.unnumbered}

From this dataset, plot brain size (**ECV**) as a function of social group size (**Group_size**), longevity (**Longevity**), juvenile period length (**Weaning**), and reproductive lifespan (**Repro_lifespan**).

```{r}
#| warning: false
#| message: false
library(cowplot)
p1 <- ggplot(data = d, aes(x=Group_size, y=ECV)) + geom_point()
p2 <- ggplot(data = d, aes(x=Longevity, y=ECV)) + geom_point()
p3 <- ggplot(data = d, aes(x=Weaning, y=ECV)) + geom_point()
p4 <- ggplot(data = d, aes(x=Repro_lifespan, y=ECV)) + geom_point()
plot_grid(p1, p2, p3, p4, nrow = 2)
```

> **NOTE:** It looks like most of these variables should probably be log transformed, though that is not essential for this exercise ;)

### Step 3 {.unnumbered}

Derive by hand the ordinary least squares regression coefficients $\beta1$ and $\beta0$ for ECV as a function of social group size.

```{r}
d_mod <- d |> filter(!is.na(ECV) & !is.na(Group_size))
# or
d_mod <- d |> drop_na(ECV, Group_size)

(b1 <- cor(d_mod$ECV, d_mod$Group_size) * sd(d_mod$ECV)/sd(d_mod$Group_size))
(b0 <- mean(d_mod$ECV) - b1 * mean(d_mod$Group_size))
```

### Step 4 {.unnumbered}

Confirm that you get the same results using the `lm()` function.

```{r}
m <- lm(ECV ~ Group_size, data = d_mod)
results <- m |>
  summary() |>
  tidy()
results
```

### Step 5 {.unnumbered}

Repeat the analysis above for three different major radiations of primates - "catarrhines", "platyrrhines", and "strepsirhines") separately. Do your regression coefficients differ among groups? How might you determine this?

```{r}
platyrrhini <- d_mod |> filter(Taxonomic_group == "Platyrrhini")
catarrhini <- d_mod |> filter(Taxonomic_group == "Catarrhini")
strepsirhini <- d_mod |> filter(Taxonomic_group == "Strepsirhini")

(platyrrhini_results <- lm(ECV ~ Group_size, data = platyrrhini) |>
  summary() |>
  tidy())

# or
(beta1_p <- cov(platyrrhini$ECV, platyrrhini$Group_size)/var(platyrrhini$Group_size))
(beta0_p <- mean(platyrrhini$ECV) - beta1_p * mean(platyrrhini$Group_size))

(catarrhini_results <- lm(ECV ~ Group_size, data = catarrhini) |>
  summary() |>
  tidy())

# or
(beta1_c <- cov(catarrhini$ECV, catarrhini$Group_size)/var(catarrhini$Group_size))
(beta0_c <- mean(catarrhini$ECV) - beta1_c * mean(catarrhini$Group_size))

(strepsirhini_results <- lm(ECV ~ Group_size, data = strepsirhini) |>
  summary() |>
  tidy())

# or
(beta1_s <- cov(strepsirhini$ECV, strepsirhini$Group_size)/var(strepsirhini$Group_size))
(beta0_s <- mean(strepsirhini$ECV) - beta1_s * mean(strepsirhini$Group_size))
```

As seen from the results above, the coefficients do differ among groups... but how might we test if this difference is signficant? One way would be to permute the group assignments randomly, for each permutation, calculate the difference in slopes between pairs of groups to create permutation distributions for that test statistic under a null model of no difference between groups. We could then compare the observed difference in slopes between groups to the "expected" distribution under that specified null model.

### Step 6 {.unnumbered}

For your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the *p* value associated with this coefficient by hand.

```{r}
# first define alpha and degrees of freedom for the regression
alpha <- 0.05
p.lower <- alpha/2
p.upper <- 1 - (alpha/2)
n <- nrow(d_mod) # number of observations
df <- n - 2

# then, calculate residuals...
residuals <- d_mod$ECV - (b0 + b1 * d_mod$Group_size)
# or residuals <- m$residuals

# then, calculate the SE for b1... 
SSE <- sum(residuals^2)
dfe <- nrow(d_mod) - 1 - 1 # number of observations - number of predictors - 1 = n - p - 1
MSE <- SSE/dfe
SSX <- sum((d_mod$Group_size - mean(d_mod$Group_size))^2) 

(SE_b1 <- sqrt(MSE/SSX))

# we can calculate an SE for b0 as well...
(SE_b0 <- SE_b1 * sqrt(sum(d_mod$Group_size^2)/n))

# calculated 95% CI for b1 assuming a t distribution...
(CI_b1 <- b1 + c(-1, 1) * qt(p = 1 - (alpha/2), df = df) * SE_b1)

# we can calculate a CI for b0 as well...
(CI_b0 <- b0 + c(-1, 1) * qt(p = 1 - (alpha/2), df = df) * SE_b0)

# calculate p values...
# first, we need t statistics...
t_b1 = b1/SE_b1
t_b0 = b0/SE_b0

(p_b1 <- pt(-1 * abs(t_b1), df = df, lower.tail = TRUE) + (1 - pt(abs(t_b1), df = df, lower.tail = TRUE)))
# or
(p_b1 <- 2 * pt(abs(t_b1), df = df, lower.tail = FALSE))

(p_b0 <- 2 * pt(abs(t_b0), df = df, lower.tail = FALSE))
```

Also extract this same information from the results of running the `lm()` function. Compare the hand-calculated output above to that pulled from the model result object, **m**.

```{r}
(results <- m |>
  summary() |>
  tidy(conf.int = TRUE, conf.level = 1 - alpha))
```

### Step 7 {.unnumbered}

Then, use a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient.

```{r}
#| warning: false
#| message: false
library(mosaic)
library(infer)
# using a loop to get a permutation distribution for slope
nperm <- 1000
perm <- vector(length = nperm)
perm.sample <- d_mod
for (i in 1:nperm){
  perm.sample$Group_size <- sample(perm.sample$Group_size)
  result <- lm(ECV ~ Group_size, data = perm.sample) |>
    tidy() |>
    filter(term == "Group_size") |>
    pull(estimate)
  perm[[i]] <- result
}
histogram(perm, xlim = c(-3,3))
ladd(panel.abline(v = b1, lty = 3, lwd = 2))
# calculate se as sd of permutation distribution
perm.se <- sd(perm)

# or, using the {infer} workflow...
perm <- d_mod |>
  # specify model
  specify(ECV ~ Group_size) |>
  # use a null hypothesis of independence
  hypothesize(null = "independence") |>
  # generate permutation replicates
  generate(reps = nperm, type = "permute") |>
  # calculate the slope statistic
  calculate(stat = "slope")

visualize(perm) + shade_p_value(obs_stat = b1, direction = "two_sided")
# calculate se as sd of permutation distribution
perm.se <- sd(perm$stat)
```

What is the p value associated with your original slope coefficient? You can use either the percentile method (i.e., using quantiles from actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error), or both, to calculate this p value.

```{r}
(p.percentile <- perm |> 
  mutate(test = abs(stat) >= abs(b1)) |>
  summarize(p = mean(test)) |>
  pull(p))
# p value taken directly from the permutation distribution i.e., how many of the slope estimates from permuted samples are more extreme than the actual b1?
# here, absolute value is used to make this a 2-tailed test

# or, using the {infer} package...
(p.percentile <- perm |> get_p_value(obs_stat = b1, direction="two_sided"))

(p.theory <- 2 * pt(abs(b1)/perm.se, df = df, lower.tail = FALSE))
# the t statistic used in `pt()` to calculate the p values is `b1/se_b1`, where `se_b1` is estimated as the standard deviation of the permutation distribution (i.e., `1/perm_se`)
# the `2 *` the upper-tail probability in this calculation is to make this a 2-tailed test
```

### Step 8 {.unnumbered}

Use bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the quantile method and the theory-based method (i.e., based on the standard deviation of the bootstrapped sampling distribution). Do these CIs suggest that your slope coefficient is different from zero?

```{r}
# using a loop to get a bootstrap distribution to generate a CI for the slope coefficient
nboot <- 1000
boot <- vector(length = nboot)
for (i in 1:nboot){
  boot.sample <- sample_n(d_mod, nrow(d_mod), replace = TRUE)
  result <- lm(ECV ~ Group_size, data = boot.sample) |>
    tidy() |>
    filter(term == "Group_size") |>
    pull(estimate)
  boot[[i]] <- result
}
histogram(boot, xlim=c(b1-3, b1+3))
CI.quantile <- c(quantile(boot, p.lower), quantile(boot, p.upper))
ladd(panel.abline(v = CI.quantile, lty = 3, lwd = 2, col = "red"))
CI.theory <- b1 + c(-1, 1) * qt(p.upper, df = df) * sd(boot)
ladd(panel.abline(v = CI.theory, lty = 3, lwd = 2, col = "blue"))

# or, using the {infer} workflow...
boot.slope <- d_mod |>
  # specify model
  specify(ECV ~ Group_size) |>
  # generate bootstrap replicates
  generate(reps = 1000, type = "bootstrap") |>
  # calculate the slope statistic
  calculate(stat = "slope")

(CI.quantile <- c(quantile(boot.slope$stat, p.lower), quantile(boot.slope$stat, p.upper)))
# or...
(CI.quantile <- get_ci(boot.slope, level = 1 - alpha, type = "percentile", point_estimate = b1))

(CI.theory <- b1 + c(-1, 1) * qt(p.upper, df = df) * sd(boot.slope$stat))
# or...
(CI.theory <- get_ci(boot.slope, level = 1 - alpha, type = "se", point_estimate = b1))
# note... the CI_theory values calculated with `get_ci()` differ very slightly from those calculated by hand because {infer} seems to be using upper/lower 0.025% boundaries of a *normal* rather than a *t* distribution
# I believe this is an "error" in how `get_ci()` is implemented in {infer}

visualize(boot.slope) +
  shade_ci(endpoints = CI.quantile, color = "red", fill = NULL) +
  shade_ci(endpoints = CI.theory, color = "blue", fill = NULL)
```

In all cases, none of these estimated CIs include zero, suggesting that the slope coefficient estimated in our linear model is "significant".

```{r include=FALSE}
detach(package:infer)
detach(package:mosaic)
detach(package:cowplot)
detach(package:broom)
detach(package:tidyverse)
```
